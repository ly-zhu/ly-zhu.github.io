<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Lingyu Zhu</title>
      <meta name="author" content="Lingyu Zhu">
      <meta name="viewport" content="width=device-width, initial-scale=1">
  
      <link rel="stylesheet" type="text/css" href="assets/css/stylesheet.css">
      <link rel="icon" type="image/png" href="images/lingyuzhu_circle.png">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
      	<td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Lingyu Zhu</name>
                </p>
                <p>Hey, I am a doctoral student at <a href="https://research.tuni.fi/vision/">Computer Vision Group, Tampere University</a>, where I am working with and advised by Prof. <a href="http://esa.rahtu.fi/">Esa Rahtu</a> on audio-visual learning and multi-model machine learning. I have been lucky to work as a visiting researcher in the <a href="http://group.iiis.tsinghua.edu.cn/~marslab/#/">MARS Lab</a> at Tsinghua University and Shanghai QiZhi Institute, advised by Prof. <a href="https://hangzhaomit.github.io/">Hang Zhao</a>.
                </p>
                <p>I completed my master thesis at Tampere University of Technology with Prof. <a href="https://scholar.google.com/citations?user=8_du6qgAAAAJ&hl=en">Heikki Huttunen</a> on data engineering and signal processing in 2017. Prior to doctoral study, I worked as a researcher at Nokia Technologies till 2019 and was advised by Dr. <a href="https://thwanguk.github.io">Tinghuai Wang</a> and Prof. <a href="https://webpages.tuni.fi/vision/public_pages/JoniKamarainen/index.html">Joni Kamarainen</a>.
                </p>
                Contact info:
                <br>
                Address: TC 307, Korkeakoulunkatu 7, FI-33720, Tampere, Finland
                <br>
                Email: firstname.lastname(at)tuni(dot)fi
                </p>
                <p style="text-align:center">
                  <!--
                  <a href="https://scholar.google.com/citations?user=RIZgZ0AAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/ly-zhu"> Github </a> &nbsp/&nbsp
                  <a href="https://fi.linkedin.com/in/lingyu-zhu-247794123/"> LinkedIn </a>
                  -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/lingyuzhu.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/lingyuzhu_circle.png" class="hoverZoomLink"></a>
                <p style="text-align:center">
                    <a href="https://scholar.google.com/citations?user=RIZgZ0AAAAAJ&hl=en">Google Scholar</a>
                    <br>
                    <a href="https://www.linkedin.com/in/lingyu-zhu-247794123/">LinkedIn</a>
                </p>
              </td>
            </tr>
          </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
	<p>I have a broad interest in computer vision with a focus on multi-model machine learning, audio-visual learning, 3D perception, embodied AI navigation, self-supervision, and semantic segmentation. Recent published papers are presented as follows.
        </p>
	</td>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="V-SlowFast/figures/fig1.png" alt="blind-date" width="160">
        </td>
        <td width="95%" valign="middle">
          <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Zhu_V-SlowFast_Network_for_Efficient_Visual_Sound_Separation_WACV_2022_paper.pdf">
            <papertitle>V-SlowFast Network for Efficient Visual Sound Separation</papertitle> &nbsp <font color="red"><strong>New!</strong></font>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2022
          <br>
          <a href="https://ly-zhu.github.io/V-SlowFast">Project</a> |
          <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Zhu_V-SlowFast_Network_for_Efficient_Visual_Sound_Separation_WACV_2022_paper.pdf">Paper</a>
          <p>
          In this paper, we propose a new light yet efficient three-stream framework V-SlowFast that operates on Visual frame, Slow spectrogram, and Fast spectrogram for visual sound separation. The Slow spectrogram captures the coarse temporal resolution while the Fast spectrogram contains the fine-grained temporal resolution.
          </p>
        </td>
      </tr>
    </tbody></tbody>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="self-supervised-motion-representations/figures/AME.png" alt="blind-date" width="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Zhu_Visually_Guided_Sound_Source_Separation_and_Localization_Using_Self-Supervised_Motion_WACV_2022_paper.pdf">
            <papertitle>Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations</papertitle> &nbsp <font color="red"><strong>New!</strong></font>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2022
          <br>
          <a href="https://ly-zhu.github.io/self-supervised-motion-representations">Project</a> |
          <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Zhu_Visually_Guided_Sound_Source_Separation_and_Localization_Using_Self-Supervised_Motion_WACV_2022_paper.pdf">Paper</a>
          <p>
          In this paper, we introduce a two-stage visual sound source separation architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. We propose an Audio-Motion Embedding (AME) framework to learn the motions of sounds in a self-supervised manner. Furthermore, we design a new Audio-Motion Transformer (AMT) module to facilitate the fusion of audio and motion cues.
          </p>
        </td>
      </tr>
    </tbody></tbody>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="leveraging-category-information-for-single-frame-visual-sound-source-separation/figures/locSep_vis_MUSIC.png" alt="blind-date" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/9484036">
            <papertitle>Leveraging Category Information for Single-Frame Visual Sound Source Separation</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <em>European Workshop on Visual Information Processing (EUVIP)</em>, 2021 &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
          <br>
          <a href="https://ly-zhu.github.io/leveraging-category-information-for-single-frame-visual-sound-source-separation">Project</a> |
          <a href="https://ieeexplore.ieee.org/abstract/document/9484036">Paper</a> |
          <a href="https://github.com/ly-zhu/Leveraging-Category-Information-for-Single-Frame-Visual-Sound-Source-Separation">Code</a>
          <p>
          We study simple yet efficient models for visual sound separation using only a single video frame. Furthermore, our models are able to exploit the information of the sound source category in the separation process. To this end, we propose two models where we assume that i) the category labels are available at the training time, or ii) we know if the training sample pairs are from the same or different category.
          </p>
        </td>
      </tr>
    </tbody></tbody>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="cof-net/figures/of.png" alt="blind-date" width="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Zhu_Visually_Guided_Sound_Source_Separation_using_Cascaded_Opponent_Filter_Network_ACCV_2020_paper.pdf">
            <papertitle>Visually Guided Sound Source Separation using Cascaded Opponent Filter Network</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <em>Asian Conference on Computer Vision (ACCV)</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
          <br>
          <a href="https://ly-zhu.github.io/cof-net">Project</a> |
          <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Zhu_Visually_Guided_Sound_Source_Separation_using_Cascaded_Opponent_Filter_Network_ACCV_2020_paper.pdf">Paper</a> |
          <a href="https://github.com/ly-zhu/cof-net">Code</a>
          <p>
          We present a system for efficient refining sound separation based on appearance and motion information of <strong>all</strong> sound sources, and localizing sound sources at pixel level.
          </p>
        </td>
      </tr>
    </tbody></tbody>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="meshnet/figures/meshnet.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_Cross-Granularity_Attention_Network_for_Semantic_Segmentation_ICCVW_2019_paper.pdf">
            <papertitle>Cross-Granularity Attention Network for Semantic Segmentation</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu*</strong>,  Tinghuai Wang*, Emre Aksu, Joni-Kristian Kamarainen
          <br>
          <em>Proceedings of the IEEE International Conference on Computer Vision Workshops</em>, 2019
          <br>
          <a href="https://ly-zhu.github.io/meshnet/">Project</a> |
          <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Zhu_Visually_Guided_Sound_Source_Separation_using_Cascaded_Opponent_Filter_Network_ACCV_2020_paper.pdf">Paper</a>
          <p>
          By integrating the cross-granularity contour enhancement into the cross-granularity categorical attention, we achieve better semantic coherence and boundary delineation.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="portrait/figures/portrait.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/8785021">
            <papertitle>Portrait Instance Segmentation for Mobile Devices</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu*</strong>,  Tinghuai Wang*, Emre Aksu, Joni-Kristian Kamarainen
          <br>
          <em>2019 IEEE International Conference on Multimedia and Expo (ICME)</em>, 2019
          <br>
          <a href="https://ly-zhu.github.io/portrait/">Project</a> |
          <a href="https://ieeexplore.ieee.org/abstract/document/8785021">Paper</a>
          <p>
          We propose an efficient non-parametric affinity model to achieve efficient instance segmentation on mobile devices.
          </p>
        </td>
      </tr>

     <!--
     <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="patent.jpg" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">  
          <a href="https://patents.google.com/patent/WO2019141896A1/en?inventor=Lingyu+Zhu&type=PATENT&num=100&oq=inventor:(Lingyu+Zhu)+type:PATENT&sort=new&dups=language">
            <papertitle>A method for neural networks</papertitle>
          </a>
          <br>
          Tinghuai Wang, Emre Aksu, <strong>Lingyu Zhu*</strong>
          <br>
          <em>WO2019141896A1, 2019</em>, 2019
          <br>
          <p>
          A method for neural networks.
          </p>
        </td>
      </tr>
	
     <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="patent.jpg" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://patents.google.com/patent/US11055852B2/en?inventor=Lingyu+zHu&type=PATENT&num=100&oq=inventor:(Lingyu+zHu)+type:PATENT&sort=new&dups=language">
            <papertitle>Fast Automatic Trimap Generation And Optimization For Segmentation Refinement</papertitle>
          </a>
          <br>
          Tinghuai Wang, <strong>Lingyu Zhu*</strong>
          <br>
          <em>US11055852B2, 2019</em>, 2019
          <br>
          <p>
          A method for fast automatic trimap generation and optimization for segmentation refinement.
          </p>
        </td>
      </tr>
      -->

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="crnn/figures/crnn.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://link.springer.com/chapter/10.1007/978-981-10-5122-7_139">
            <papertitle>Predicting Gene Expression Levels from Histone Modification Signals with Convolutional Recurrent Neural Networks</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>,  Juha Kesseli, Matti Nykter, Heikki Huttunen
          <br>
          <em>EMBEC & NBC</em>, 2017
          <br>
          <a href="https://ly-zhu.github.io/crnn/">Project</a> |
          <a href="https://link.springer.com/chapter/10.1007/978-981-10-5122-7_139">Paper</a> |
          <a href="https://github.com/ly-zhu/CRNN-gene-expression-with-histone-modifications">Code</a>
          <p>
          This paper studies how a Convolutional Recurrent Neural Network performs on predicting the gene expression levels from histone modification signals.
          </p>
        </td>
      </tr>
    </tbody></tbody>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
      <tr>
        <td width="100%" valign="middle">
          <heading>Teaching</heading>
        <p>[Tampere University of Technology] SGN-41007 Pattern Recognition and Machine Learning (Teaching Assistant)
        </p>
        <p>[Tampere University of Technology] SGN-84007 Introduction to Matlab (Teaching Assistant)
        </p>
        </td>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
      <tr>
        <td width="100%" valign="middle">
          <heading>Activities</heading>
        <p>[Junction 2017, Helsinki, Finland] 1st place from the FleetBoard challenge provided by Daimler
        </p>
        <p>[Kaggle competition: Gene Expression Prediction] Organizer of the Kaggle competition
        </p>
        <p>[Kaggle competition: TUT Copper Analysis Challenge] 3rd place in this Kaggle challenge
        </p>
        <p>[Kaggle competition: TUGraz-TUT Face Verification Challenge] 3rd place in this Kaggle challenge
        </p> 
        </td>
    </table>


    </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <br>
            <p align="right">
              <font size="2">
              Template <a href="https://jonbarron.info/"><strong>link</strong></a>.
              </font>
            </p>
          </td>
        </tr>
    </table>

    <!-- {% if site.google_analytics %} -->
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168697816-1"></script>
      <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-168697816-1');
      </script>
    <!-- {% endif %} -->
    
  </body>
</html>
