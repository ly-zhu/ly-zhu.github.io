<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Lingyu Zhu</title>
      <meta name="author" content="Lingyu Zhu">
      <meta name="viewport" content="width=device-width, initial-scale=1">
  
      <link rel="stylesheet" type="text/css" href="assets/css/stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
      	<td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Lingyu Zhu</name>
                </p>
                <p>Hey, I am a doctoral student at <a href="https://research.tuni.fi/vision/">Computer Vision Group, Tampere University</a>, where I am working with Prof. <a href="http://esa.rahtu.fi/">Esa Rahtu</a>.
                </p>
                <p>I completed my master thesis at Tampere University of Technology with Prof. <a href="https://scholar.google.com/citations?user=8_du6qgAAAAJ&hl=en">Heikki Huttunen</a> on data engineering and signal processing in 2017. Prior to doctoral study, I worked as a researcher at Nokia Technologies till 2019 and was advised by Dr. <a href="https://thwanguk.github.io">Tinghuai Wang</a> and Prof. <a href="https://webpages.tuni.fi/vision/public_pages/JoniKamarainen/index.html">Joni Kamarainen</a>.
                </p>
                Contact info:
                <br>
                Address: TC 307, Korkeakoulunkatu 7, FI-33720, Tampere, Finland
                <br>
                Email: firstname.lastname(at)tuni(dot)fi
                </p>
                <p style="text-align:center">
                  <!--
                  <a href="https://scholar.google.com/citations?user=RIZgZ0AAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/ly-zhu"> Github </a> &nbsp/&nbsp
                  <a href="https://fi.linkedin.com/in/lingyu-zhu-247794123/"> LinkedIn </a>
                  -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/lingyuzhu.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/lingyuzhu_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
      <tr>
        <td width="100%" valign="middle">
          <heading>Projects</heading>
	<p>My research interest is in computer vision with a focus on audio-visual learning, self-supervision, and semantic segmentation.
        </p>
	</td>
    </table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="self-supervised-motion-representations/figures/AME.png" alt="blind-date" width="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2104.08506.pdf">
            <papertitle>Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <a href="https://ly-zhu.github.io/self-supervised-motion-representations">project</a> |
          <a href="https://arxiv.org/pdf/2104.08506.pdf">paper</a>
          <p>
          In this paper, we introduce a two-stage visual sound source separation architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. We propose an Audio-Motion Embedding (AME) framework to learn the motions of sounds in a self-supervised manner. Furthermore, we design a new Audio-Motion Transformer (AMT) module to facilitate the fusion of audio and motion cues.
          </p>
        </td>
      </tr>
    </tbody></tbody>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="leveraging-category-information-for-single-frame-visual-sound-source-separation/figures/locSep_vis_MUSIC.png" alt="blind-date" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2007.07984.pdf">
            <papertitle>Leveraging Category Information for Single-Frame Visual Sound Source Separation</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
	  <em>European Workshop on Visual Information Processing (EUVIP)</em>, 2021 (<strong>BEST PAPER AWARD</strong>)
          <br>
          <a href="https://ly-zhu.github.io/leveraging-category-information-for-single-frame-visual-sound-source-separation">project</a> |
          <a href="https://arxiv.org/pdf/2007.07984.pdf">paper</a>
          <p>
          We study simple yet efficient models for visual sound separation using only a single video frame. Furthermore, our models are able to exploit the information of the sound source category in the separation process. To this end, we propose two models where we assume that i) the category labels are available at the training time, or ii) we know if the training sample pairs are from the same or different category.
          </p>
        </td>
      </tr>
    </tbody></tbody>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="cof-net/figures/of.png" alt="blind-date" width="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Zhu_Visually_Guided_Sound_Source_Separation_using_Cascaded_Opponent_Filter_Network_ACCV_2020_paper.html">
            <papertitle>Visually Guided Sound Source Separation using Cascaded Opponent Filter Network</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <em>Asian Conference on Computer Vision (ACCV)</em>, 2020 (<strong>Oral</strong>)
          <br>
          <a href="https://ly-zhu.github.io/cof-net">project</a> |
          <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Zhu_Visually_Guided_Sound_Source_Separation_using_Cascaded_Opponent_Filter_Network_ACCV_2020_paper.pdf">paper</a>
          <p>
          We present a system for efficient refining sound separation based on appearance and motion information of <strong>all</strong> sound sources, and localizing sound sources at pixel level.
          </p>
        </td>
      </tr>
    </tbody></tbody>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="meshnet/figures/meshnet.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_Cross-Granularity_Attention_Network_for_Semantic_Segmentation_ICCVW_2019_paper.pdf">
            <papertitle>Cross-Granularity Attention Network for Semantic Segmentation</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>,  Tinghuai Wang, Emre Aksu, Joni-Kristian Kamarainen
          <br>
          <em>Proceedings of the IEEE International Conference on Computer Vision Workshops</em>, 2019
          <br>
          <a href="https://ly-zhu.github.io/meshnet">project</a> |
          <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_Cross-Granularity_Attention_Network_for_Semantic_Segmentation_ICCVW_2019_paper.pdf">paper</a>
          <p>
          By integrating the cross-granularity contour enhancement into the cross-granularity categorical attention, we achieve better semantic coherence and boundary delineation.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="portrait/figures/portrait.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/8785021">
            <papertitle>Portrait Instance Segmentation for Mobile Devices</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>,  Tinghuai Wang, Emre Aksu, Joni-Kristian Kamarainen
          <br>
          <em>2019 IEEE International Conference on Multimedia and Expo (ICME)</em>, 2019
          <br>
          <a href="https://ly-zhu.github.io/portrait">project</a> |
          <a href="https://ieeexplore.ieee.org/abstract/document/8785021">paper</a>
          <p>
          We propose an efficient non-parametric affinity model to achieve efficient instance segmentation on mobile devices.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="crnn/figures/crnn.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://link.springer.com/chapter/10.1007/978-981-10-5122-7_139">
            <papertitle>Predicting Gene Expression Levels from Histone Modification Signals with Convolutional Recurrent Neural Networks</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>,  Juha Kesseli, Matti Nykter, Heikki Huttunen
          <br>
          <em>EMBEC & NBC</em>, 2017
          <br>
          <a href="https://ly-zhu.github.io/crnn">project</a> |
          <a href="https://link.springer.com/chapter/10.1007/978-981-10-5122-7_139">paper</a>
          <p>
          This paper studies how a Convolutional Recurrent Neural Network performs on predicting the gene expression levels from histone modification signals.
          </p>
        </td>
      </tr>
    </tbody></tbody>

    </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <br>
            <p align="right">
              <font size="2">
              Template <a href="https://jonbarron.info/"><strong>link</strong></a>.
              </font>
            </p>
          </td>
        </tr>
    </table>

    <!-- {% if site.google_analytics %} -->
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168697816-1"></script>
      <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-168697816-1');
      </script>
    <!-- {% endif %} -->
    
  </body>
</html>
