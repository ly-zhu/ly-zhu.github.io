<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Lingyu Zhu</title>
      <meta name="author" content="Lingyu Zhu">
      <meta name="viewport" content="width=device-width, initial-scale=1">
  
      <link rel="stylesheet" type="text/css" href="assets/css/stylesheet.css">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
      	<td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p style="text-align:center">
                  <name>Lingyu Zhu</name>
                </p>
                <p>Hey, I am a Ph.D. student at <a href="https://research.tuni.fi/vision/">Computer Vision Group, Tampere University</a> under the supervision of Professor <a href="http://esa.rahtu.fi/">Esa Rahtu</a>. My research interests include computer vision, cross modal machine learning and semantic segmentation.
                </p>
                <p>I completed my master thesis at Tampere University of Technology with Professor <a href="http://www.cs.tut.fi/~hehu/">Heikki Huttunen</a> on data engineering in 2017. Prior to doctoral study, I worked as an external researcher at Nokia Technologies till 2019 and was advised by Dr. <a href="https://sites.google.com/site/tinghuaiw/">Tinghuai Wang</a> and Professor <a href="http://vision.cs.tut.fi/personal/JoniKamarainen/">Joni Kamarainen</a>.
                </p>
                Contact info
                <br>
                Address: TC 307, Korkeakoulunkatu 7, FI-33720, Tampere, Finland
                <br>
                Email: firstname.lastname(at)tuni(dot)fi
                </p>
                <p style="text-align:center">
                  <!--
                  <a href="https://scholar.google.com/citations?user=RIZgZ0AAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/ly-zhu"> Github </a> &nbsp/&nbsp
                  <a href="https://fi.linkedin.com/in/lingyu-zhu-247794123/"> LinkedIn </a>
                  -->
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/lingyuzhu.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/lingyuzhu_circle.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
      <tr>
        <td width="100%" valign="middle">
          <heading>Projects</heading>
        </td>    
    </table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="separating-sounds-from-single-image/figures/locSep_vis_MUSIC.png" alt="blind-date" width="160" height="100">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2007.07984.pdf">
            <papertitle>Separating Sounds from a Single Image</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <a href="https://ly-zhu.github.io/separating-sounds-from-single-image">project</a> |
          <a href="https://arxiv.org/pdf/2007.07984.pdf">paper</a>
          <p>
          An efficient appearance attention module is introduced to improve the sound separation performance and to precisely locate sound sources. Moreover, we utilize the ground category information to study the capacity of each sub-network.
          </p>
        </td>
      </tr>
    </tbody></tbody>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="cof-net/figures/of.png" alt="blind-date" width="160">
        </td>
        <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2006.03028.pdf">
            <papertitle>Visually Guided Sound Source Separation using Cascaded Opponent Filter Network</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>, Esa Rahtu
          <br>
          <a href="https://ly-zhu.github.io/cof-net">project</a> |
          <a href="https://arxiv.org/pdf/2006.03028.pdf">paper</a>
          <p>
          We present a system for efficient refining sound separation based on appearance and motion information, and localizing sound sources at pixel level.
          </p>
        </td>
      </tr>
    </tbody></tbody>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="meshnet/figures/meshnet.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_Cross-Granularity_Attention_Network_for_Semantic_Segmentation_ICCVW_2019_paper.pdf">
            <papertitle>Cross-Granularity Attention Network for Semantic Segmentation</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>,  Tinghuai Wang, Emre Aksu, Joni-Kristian Kamarainen
          <br>
          <em>Proceedings of the IEEE International Conference on Computer Vision Workshops</em>, 2019
          <br>
          <a href="https://ly-zhu.github.io/meshnet">project</a> |
          <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_Cross-Granularity_Attention_Network_for_Semantic_Segmentation_ICCVW_2019_paper.pdf">paper</a>
          <p>
          By integrating the cross-granularity contour enhancement into the cross-granularity categorical attention we can achieve better semantic coherence and boundary delineation.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="portrait/figures/portrait.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/8785021">
            <papertitle>Portrait Instance Segmentation for Mobile Devices</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>,  Tinghuai Wang, Emre Aksu, Joni-Kristian Kamarainen
          <br>
          <em>2019 IEEE International Conference on Multimedia and Expo (ICME)</em>, 2019
          <br>
          <a href="https://ly-zhu.github.io/portrait">project</a> |
          <a href="https://ieeexplore.ieee.org/abstract/document/8785021">paper</a>
          <p>
          We propose an efficient non-parametric affinity model to achieve efficient instance segmentation on mobile devices.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="crnn/figures/crnn.png" alt="blind-date" width="160" height="120">
        </td>
        <td width="75%" valign="middle">
          <a href="https://link.springer.com/chapter/10.1007/978-981-10-5122-7_139">
            <papertitle>Predicting Gene Expression Levels from Histone Modification Signals with Convolutional Recurrent Neural Networks</papertitle>
          </a>
          <br>
          <strong>Lingyu Zhu</strong>,  Juha Kesseli, Matti Nykter, Heikki Huttunen
          <br>
          <em>EMBEC & NBC</em>, 2017
          <br>
          <a href="https://ly-zhu.github.io/crnn">project</a> |
          <a href="https://link.springer.com/chapter/10.1007/978-981-10-5122-7_139">paper</a>
          <p>
          This paper studies how a Convolutional Recurrent Neural Network performs for predicting the gene expression levels from histone modification signals.
          </p>
        </td>
      </tr>
    </tbody></tbody>

    </table>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
            <br>
            <p align="right">
              <font size="2">
              Template <a href="https://jonbarron.info/"><strong>link</strong></a>.
              </font>
            </p>
          </td>
        </tr>
    </table>

    <!-- {% if site.google_analytics %} -->
      <!-- Global site tag (gtag.js) - Google Analytics -->
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-168697816-1"></script>
      <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-168697816-1');
      </script>
    <!-- {% endif %} -->
    
  </body>
</html>
