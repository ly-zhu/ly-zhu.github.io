<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>self-supervised-motion-representations</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="icon" type="image/png" href="https://ly-zhu.github.io/images/lingyuzhu_circle.png">

    <link rel="stylesheet" href="https://ly-zhu.github.io/assets/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://ly-zhu.github.io/assets/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://ly-zhu.github.io/assets/css/codemirror.min.css">
    <link rel="stylesheet" href="https://ly-zhu.github.io/assets/css/app.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="https://ly-zhu.github.io/assets/js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b> Visually Guided Sound Source Separation and Localization using Self-Supervised Motion Representations</b>
                <small>
                </small>
            </h2>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://ly-zhu.github.io/"> Lingyu Zhu </a>
                        </br> Tampere University
                    </li>
                    
                    <li>
                        <a href="https://esa.rahtu.fi/"> Esa Rahtu </a>
                        </br> Tampere University
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Zhu_Visually_Guided_Sound_Source_Separation_and_Localization_Using_Self-Supervised_Motion_WACV_2022_paper.pdf">
                            <image src="figures/paper_img.png" height="120px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>

                        <li>
                        <a href="https://github.com/ly-zhu/self-supervised-motion-representations">
                        <image src="https://ly-zhu.github.io/images/github_icon.png" height="120px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overviw video
                </h3>
                <video id="v0" width="100%" autoplay loop muted controls>
                  <source src="figures/supp.mp4" type="video/mp4" />
                </video>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    The objective of this paper is to perform audio-visual sound source separation, i.e. to separate component audios from a mixture based on the videos of sound sources. Moreover, we aim to pinpoint the source location in the input video sequence. Recent works have shown impressive audio-visual separation results when using prior knowledge of the source type (e.g. human playing instrument) and pre-trained motion detectors (e.g. keypoints or optical flows). However, at the same time, the models are limited to a certain application domain. In this paper, we address these limitations and make the following contributions: i) we propose a two-stage architecture, called Appearance and Motion network (AMnet), where the stages specialise to appearance and motion cues, respectively. The entire system is trained in a self-supervised manner; ii) we introduce an Audio-Motion Embedding (AME) framework to explicitly represent the motions that related to sound; iii) we propose an audio-motion transformer architecture for audio and motion feature fusion; iv) we demonstrate state-of-the-art performance on two challenging datasets (MUSIC-21 and AVE) despite the fact that we do not use any pre-trained keypoint detectors or optical flow estimators.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    The overall architecture of the proposed Appearance and Motion network (AMnet)
                </h3>
                <image src="figures/overview.png" class="img-responsive" alt="overview" width="100%" style="max-height: 450px;margin:auto;"><br>
                <p class="text-justify">
                    The overall architecture of the proposed Appearance and Motion network (AMnet). The Audio-Appearance stage encodes a video keyframe and a mixture audio spectrogram into an appearance feature vector and a spectrum feature volume, respectively. These are subsequently fused by calculating a weighted sum of spectrum features using appearance features as weights. The result is further converted to a binary mask and multiplied with the input spectrogram to produce the separated output. The Audio-Motion stage again encodes the spectrograms and the video sequence into feature representations. These are fused with the proposed Audio-Motion Transformer module, decoded, and passed to produce a refined mask, which is multiplied with the input mixture spectrogram to produce the final output.
               </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Audio-Motion Embedding Framework
                </h3>
                <image src="figures/AME.png" class="img-responsive" alt="overview" width="90%" style="margin:auto;"><br>
                <p class="text-justify">
                    The proposed Audio-Motion Embedding (AME) framework exploits the natural correlation between the audio and motion of a natural video. The AME framework (see Figure~\ref{fig:AVTA}) consists of Motion Network and Sound Network, which map the motion and audio sequences into a common embedding space, respectively. We formulate the learning objectives to enforce small embedding distances between well synchronised streams and large distances for out-of-sync streams. We hypothesize that this learning objective encourages the Motion Network to focus mainly on the sound related motion features in the input video.
                </p>
                <p class="text-justify">
                    The AME maps the motions and audio into the common embedding space. The mappings are learned in the following manner: i) given a video clip v with an aligned audio x_aligned, we generate a misaligned audio x_misaligned by randomly shifting the waveform in time domain; ii) we encode the video stream, aligned audio, and the misaligned audio into embedding representations using the motion and sound networks, respectively; iii) we calculate the distances between the video embedding and the both audio embeddings, and formulate a cost function using a triplet loss approach; and iv) we optimise the embedding networks by minimizing the loss function over a large set of videos.
                </p>
                <p class="text-justify">
                    Previous works using audio-visual synchronization for self-supervised representation learning formulate the problem as a classification task where the system decides if the given audio and video are synchronized (1) or not (0). Moreover, the information along the temporal dimension is mostly neglected by marginalizing the corresponding dimension with pooling operation. In contrast, we formulate the problem as a mapping from motion and audio domains to a common embedding space, where the distances correlate with the temporal alignment. Moreover, the representation retains the temporal dimension at the output features.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Audio-Appearance Sound Source Separation
                </h3>
                <image src="figures/eq_1.png" class="img-responsive" alt="overview" width="80%" style="max-height: 450px;margin:auto;"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Audio-Motion Sound Source Separation
                </h3>
                <image src="figures/eq_2.png" class="img-responsive" alt="overview" width="80%" style="max-height: 450px;margin:auto;"><br>
            </div>
        </div>

        <div class="row" >
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Audio-Motion Transformer
                </h3>
                <image src="figures/AMT.png" class="img-responsive" alt="overview" width="100%" style="max-height: 450px;margin:auto;"><br>
                <p class="text-justify">
                    The Audio-Motion Transformer (AMT) module is used to leverage the obtained motion cues (the motions of sound learned with AME) for sound source separation.
                </p>
            </div>
        </div>

        <div class="row" >
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Sound Source Separation Results
                </h3>
                <image src="figures/sep_diff_MUSIC21.png" class="img-responsive" alt="overview" width="100%" style="max-height: 450px;margin:auto;"><br>
            </div>
        </div>

        <div class="row" >
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Examples of Separating different type of sources
                </h3>
                <image src="figures/sep_diff.png" class="img-responsive" alt="overview" width="80%" style="max-height: 450px;margin:auto;"><br>
            </div>
        </div>

        <div class="row" >
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Examples of Separating same type of sources
                </h3>
                <image src="figures/sep_same.png" class="img-responsive" alt="overview" width="80%" style="max-height: 450px;margin:auto;"><br>
            </div>
        </div>


        <div class="row" >
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Examples of Sound Source Localization
                </h3>
                <image src="figures/loc.png" class="img-responsive" alt="overview" width="100%" style="margin:auto;"><br>
            </div>
        </div>

        <div class="row" style="position:relative;padding-top:0%;">
            <div class="col-md-8 col-md-offset-2" >
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-15 col-md-offset-0">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{zhu2022visually,
  title={Visually guided sound source separation and localization using self-supervised motion representations},
  author={Zhu, Lingyu and Rahtu, Esa},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1289--1299},
  year={2022}
}
                </textarea>
                </div>
            </div>
        </div>
    </div>
</body>
</html>
